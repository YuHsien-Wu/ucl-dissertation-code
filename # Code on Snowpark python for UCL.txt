# Code on Snowpark python for UCL
# Step 1 Dealing with outliers (0.01,0.99) and creating new table

import snowflake.snowpark as snowpark
from snowflake.snowpark.functions import col, approx_percentile, when, min as sf_min, max as sf_max, lit

def main(session: snowpark.Session): 

    # Load original table
    df = session.table("ANALYSIS_DATAFRAME")
    original_schema = {c.name: str(c.datatype).upper() for c in df.schema.fields}

    # Auto-detect numeric columns (Longtype, Decimaltype, Doubletype)
    numeric_cols = [
        c.name for c in df.schema.fields
        if (
            "longtype" in str(c.datatype).lower() or
            "decimaltype" in str(c.datatype).lower() or
            "doubletype" in str(c.datatype).lower()
        ) and c.name not in ["PIN_RN", "APPICATION_DATE", "LAST_UPDATE_DATE", "LAST_UPDATE_DATE_ORIG", "OPENDATE", "CLOSEDATE"]]
    
    capped_columns = []

    # Cap each numeric column
    for col_name in numeric_cols:
        stats = df.agg(
            approx_percentile(col(col_name),0.01).alias("LOWER"),
            approx_percentile(col(col_name),0.99).alias("UPPER")
        ).collect()[0]

        lower_bound, upper_bound = stats["LOWER"], stats["UPPER"]
        
        #if bounds are missing (e.g. column all NULLs)
        if lower_bound is None or upper_bound is None:
            capped_columns.append(col(col_name).alias(col_name))
            continue
        # Capped column
        capped_columns.append(
             when(col(col_name) < lower_bound, lower_bound)
                .when(col(col_name) > upper_bound, upper_bound)
                .otherwise(col(col_name))
                .alias(col_name)
        )
    

      
    # Add untouched columns
    untouched_columns = [col(c.name) for c in df.schema.fields if c.name not in numeric_cols]
    
    # formulating final dataframe
    final_df = df.select(*untouched_columns, *capped_columns)

    # Restore original data types for original columns
    for column_name, dtype in original_schema.items():
        if column_name in final_df.columns:
            if "VARCHAR" in dtype:
                final_df = final_df.with_column(column_name, col(column_name).cast("STRING"))
            elif "NUMBER" in dtype:
                final_df = final_df.with_column(column_name, col(column_name).cast("NUMBER")) 
            elif "FLOAT" in dtype or "DOUBLE" in dtype:
                final_df = final_df.with_column(column_name, col(column_name).cast("FLOAT"))
            elif "DATE" in dtype:
                final_df = final_df.with_column(column_name, col(column_name).cast("DATE"))

    # Cast all _OUTLIER_FLAG columns as INT
    for column_name in final_df.columns:
        if column_name.endswith("_OUTLIER_FLAG"):
            final_df = final_df.with_column(column_name, col(col_name).cast("INT"))

    # Save the cleaned dataframe
    final_df.write.mode("overwrite").save_as_table("ANALYSIS_DATAFRAME_CLEANED")            
    
    #     
    return session.table("ANALYSIS_DATAFRAME_CLEANED")


# Step 2 Create final dataframe
import snowflake.snowpark as snowpark
from snowflake.snowpark.functions import col

def main(session: snowpark.Session): 

    df = session.table("ANALYSIS_DATAFRAME_CLEANED")

    columns_to_cast_float = ["E5_S_05_2", "ND_SP_CII"]
    for c in columns_to_cast_float:
        df = df.with_column(c, col(c).cast("FLOAT"))
    
    New_df = df.select(
        col("PIN"), 
        col("LIN"), 
        col("UTILIZATION1"), 
        col("UTILIZATION2"),
        col("CREDIT_LIMIT"),
        col("STARTING_CREDIT_LIMIT"),
        col("APPLICATION_TYPE"), 
        col("E5_S_05_2"),
        col("ND_SP_CII"), 
        col("update_vs_application"), 
        col("PIN_RN"),
        col("linked_cais_flag")
                  )
    New_df.write.mode("overwrite").save_as_table("Final_table") 

    return New_df

# Step 3 Visualise before outliers removal 

import snowflake.snowpark as snowpark
from snowflake.snowpark.functions import col

def main(session: snowpark.Session): 
    # Your code goes here, inside the "main" handler.
    df = session.table("Analysis_dataframe")

    bin_vars = {
        #"P_PERSONAL_INCOME": 5000,
        "STARTING_CREDIT_LIMIT": 500,
        "CREDIT_LIMIT": 500,
        "E5_S_05_2": 100,
        "ND_SP_CII": 10,
        "UTILIZATION1": 0.1,
        "UTILIZATION2": 1
    }
    
    summary_tables = {}


    for var, bin_size in bin_vars.items():
        
        bin_index_col = ((col(var) / bin_size).cast("INT")).alias("BIN")
        grouped = (
            df.group_by(bin_index_col)
            .count()
            .order_by(col("BIN"))
            .with_column("BIN_START", col("BIN") * bin_size)
            .select(
                col("BIN"),
                col("BIN_START"),
                col("COUNT")
            )
            .with_column_renamed("COUNT", f"{var}_count")
        )
        summary_tables[var] = grouped

    
    return summary_tables["CREDIT_LIMIT"]

# Step 3-1 Visualise after outlier removal
def main(session: snowpark.Session): 
    # Your code goes here, inside the "main" handler.
    df = session.table("final_table")

    bin_vars = {
        #"P_PERSONAL_INCOME": 5000,
        "STARTING_CREDIT_LIMIT": 500,
        "CREDIT_LIMIT": 500,
        "E5_S_05_2": 100,
        "ND_SP_CII": 10
    }
    
    summary_tables = {}


    for var, bin_size in bin_vars.items():
        
        bin_index_col = ((col(var) / bin_size).cast("INT")).alias("BIN")
        
        grouped = (
            df.group_by(bin_index_col)
            .count()
            .order_by(col("BIN"))
            .with_column("BIN_START", col("BIN") * bin_size)
            .select(
                col("BIN"),
                col("BIN_START"),
                col("COUNT")
            )
            .with_column_renamed("COUNT", f"{var}_count")
        )
        summary_tables[var] = grouped

    
    return summary_tables["CREDIT_LIMIT"]

# Step 5 H1 distribution

import snowflake.snowpark as snowpark
from snowflake.snowpark.functions import col

def main(session: snowpark.Session): 
    # Your code goes here, inside the "main" handler.
    df = session.table("AP2_EXTRACT_SECOND_SNAPSHOT")

    bin_vars = {
        "E5_S_05_2_NEXT_SNAPSHOT":100,
        "E5_S_05_2" :100
    }
    
    summary_tables = {}


    for var, bin_size in bin_vars.items():
        
        bin_index_col = ((col(var) / bin_size).cast("INT")).alias("BIN")
        
        grouped = (
            df.group_by(bin_index_col)
            .count()
            .order_by(col("BIN"))
            .with_column("BIN_START", col("BIN") * bin_size)
            .select(
                col("BIN"),
                col("BIN_START"),
                col("COUNT")
            )
            .with_column_renamed("COUNT", f"{var}_count")
        )
        summary_tables[var] = grouped

    
    return summary_tables["E5_S_05_2"]

# 5-1 t-test for H1

import snowflake.snowpark as snowpark
from snowflake.snowpark.functions import col, approx_percentile, when, min as sf_min, max as sf_max, lit

def main(session: snowpark.Session): 
    import pandas as pd
    from scipy.stats import ttest_rel
    df = session.table("AP2_EXTRACT_SECOND_SNAPSHOT")

    columns_to_cast_float = ["E5_S_05_2", "E5_S_05_2_NEXT_SNAPSHOT"]
    for col_name in columns_to_cast_float:
        df = df.with_column(col_name, col(col_name).cast("FLOAT"))
    capped_columns = []
    stats = df.agg(
            approx_percentile(col(col_name),0.01).alias("LOWER"),
            approx_percentile(col(col_name),0.99).alias("UPPER")
        ).collect()[0]

    lower_bound, upper_bound = stats["LOWER"], stats["UPPER"]

        
    # Capped column
    capped_columns.append(
        when(col(col_name) < lower_bound, lower_bound)
        .when(col(col_name) > upper_bound, upper_bound)
        .otherwise(col(col_name))
        .alias(col_name)
    )
    pdf = df.select(*capped_columns)

    pdf = df.to_pandas()
    t_stat, p_value = ttest_rel(pdf["E5_S_05_2"], pdf["E5_S_05_2_NEXT_SNAPSHOT"])
    
    mean_c1 = float(pdf["E5_S_05_2"].mean())
    mean_c2 = float(pdf["E5_S_05_2_NEXT_SNAPSHOT"].mean())
    
    result_data = [(t_stat,p_value, mean_c1, mean_c2)]

    schema = ["t_statistic", "p_value", "mean_c1", "mean_c2"]

    # Print a sample of the dataframe to standard output.
    result_sdf = session.create_dataframe(result_data, schema = schema)
    return result_sdf

# T-test for H2
import snowflake.snowpark as snowpark
from snowflake.snowpark.functions import col

def main(session: snowpark.Session): 
    # Your code goes here, inside the "main" handler.
    import pandas as pd
    from scipy.stats import ttest_rel

    df = session.table("cais_extract_retro")
    df = df.filter(col("credit_limit") > 0)
    df = df.with_column("utilization1", col("bal1") / col("credit_limit"))
    df = df.with_column("utilization2", col("bal2") / col("credit_limit"))

    pdf = df.select(col("utilization1"), col("utilization2")).to_pandas()

    t_stat, p_value = ttest_rel(pdf["UTILIZATION1"], pdf["UTILIZATION2"])
    mean_u1 = float(pdf["UTILIZATION1"].mean())
    mean_u2 = float(pdf["UTILIZATION2"].mean())

    result_data = [(t_stat, p_value, mean_u1, mean_u2)]
    schema = ["t_statistic", "p_value", "mean_u1", "mean_u2"]

    # Print a sample of the dataframe to standard output.
    result_sdf = session.create_dataframe(result_data, schema = schema)
    return result_sdf

# T-test for H2 Balance
import snowflake.snowpark as snowpark
from snowflake.snowpark.functions import col

def main(session: snowpark.Session): 
    # Your code goes here, inside the "main" handler.
    import pandas as pd
    from scipy.stats import ttest_rel

    df = session.table("cais_extract_retro")
    df_bal = df.select(col("BAL1"), col("BAL2")).to_pandas()
    mean_bal1 = float(df_bal["BAL1"].mean())
    mean_bal2 = float(df_bal["BAL2"].mean())
    t_stat, p_value = ttest_rel(df_bal["BAL1"],df_bal["BAL2"])

    result_data = [(t_stat, p_value, mean_bal1, mean_bal2)]
    schema = ["t_statistic", "p_value", "mean_bal1", "mean_bal2"]

    # Print a sample of the dataframe to standard output.
    result_sdf = session.create_dataframe(result_data, schema = schema)
    return result_sdf


# H3 utilisation Distribution
import snowflake.snowpark as snowpark
from snowflake.snowpark.functions import col, when, lit, avg, count, call_function

def main(session: snowpark.Session): 
    base = session.table("Final_table").filter(col("PIN_RN") == 1)


    approvals = base.filter(col("LINKED_CAIS_FLAG") == 1)
    p10_row = approvals.agg(call_function("APPROX_PERCENTILE", col("STARTING_CREDIT_LIMIT").cast("float"),0.10).alias("P10")).collect()[0]["P10"]
    p10_c = lit(p10_row)

    is_rejected = (col("LINKED_CAIS_FLAG") == 0)
    is_approved = (col("LINKED_CAIS_FLAG") == 1)
    low_limit = (col("STARTING_CREDIT_LIMIT") <= p10_c)
    worsened = (col("UTILIZATION1") < col("UTILIZATION2"))
    improved = (col("UTILIZATION1") > col("UTILIZATION2"))
    unchanged = (col("UTILIZATION1") == col("UTILIZATION2"))
    
    df = (
        base.with_column(
            "category",
            when(is_rejected & improved, lit("rejected_potentially_good"))
            .when(is_approved & low_limit & worsened, lit("approved_lowlimit_potentially_bad"))
            .when(is_approved & (~ low_limit) & worsened, lit("approved_normallimit_potentially_bad"))
            .otherwise(lit("other"))
        )
    )

    explained = df.with_column(
        "Other_reason",
        when(is_rejected & worsened & (col("category") == lit("other")), lit("rejected_worsened"))
        .when(is_rejected & unchanged & (col("category") == lit("other")), lit("rejected_unchanged"))
        .when(is_approved & improved & (col("category") == lit("other")), lit("approved_improved"))
        .when(is_approved & unchanged & (col("category") == lit("other")), lit("approved_unchanged"))
        .otherwise(lit(None))
    )

        
    summary_df = (
        explained.group_by("category", "Other_reason")
        .count()
        .sort(col("count").desc())
    )

    return summary_df

# T-test for H3
import snowflake.snowpark as snowpark
from snowflake.snowpark.functions import col

def main(session: snowpark.Session): 
    import pandas as pd
    from scipy.stats import ttest_rel

    
    df = session.table("final_table").filter(col("PIN_RN") == 1).filter(col("LINKED_CAIS_FLAG") == 1)
    pdf = df.select(col("utilization1"), col("utilization2")).to_pandas()

    t_stat, p_value = ttest_rel(pdf["UTILIZATION1"], pdf["UTILIZATION2"])
    mean_u1 = float(pdf["UTILIZATION1"].mean())
    mean_u2 = float(pdf["UTILIZATION2"].mean())

    result_data = [(t_stat, p_value, mean_u1, mean_u2)]
    schema = ["t_statistic", "p_value", "mean_u1", "mean_u2"]

    # Print a sample of the dataframe to standard output.
    result_sdf = session.create_dataframe(result_data, schema = schema)
    return result_sdf


# H3 distribution
import snowflake.snowpark as snowpark
from snowflake.snowpark.functions import col, when, lit, avg, count, call_function

def main(session: snowpark.Session): 
    base = session.table("Final_table").filter(col("PIN_RN") == 1)
    
    approvals = base.filter(col("LINKED_CAIS_FLAG") == 1)
    p10_row = approvals.agg(call_function("APPROX_PERCENTILE", col("STARTING_CREDIT_LIMIT").cast("float"),0.10).alias("P10")).collect()[0]["P10"]
    p10_c = lit(p10_row)
    
    is_rejected = (col("LINKED_CAIS_FLAG") == 0)
    is_approved = (col("LINKED_CAIS_FLAG") == 1)
    low_limit = (col("STARTING_CREDIT_LIMIT") <= p10_c)

    df = (
        base.with_column(
            "category",
            when(is_rejected, lit("Rejected"))
            .when(is_approved & low_limit, lit("New Account - Low Credit"))
            .when(is_approved & (~ low_limit), lit("New Account - High Credit "))
            .otherwise(lit("other"))
        )
    )
    df = df.with_column(
        "Cohort",
        when((col("UPDATE_VS_APPLICATION") == "pre"), lit("Cohort 1(Pre)"))
        .when((col("UPDATE_VS_APPLICATION") == "post"), lit("Cohort 2(Post)"))        
    )

    result_df = (
        df.group_by("Cohort", "category")
        .agg(count(lit(1))).alias("Customer_count")
        .sort(col("cohort"), col("category"))
    )
    
    return result_df
